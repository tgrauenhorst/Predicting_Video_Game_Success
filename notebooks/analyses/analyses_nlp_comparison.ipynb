{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3fd0f4-12f3-4bd7-9c98-3d80d6cb845c",
   "metadata": {},
   "source": [
    "# Notebook: Basic NLP Analyses v2\n",
    "\n",
    "## Structure of notebook\n",
    "#### Step 1: Imports, functions and classes\n",
    "#### Step 2: Data preparation\n",
    "#### Step 3: NLP basics\n",
    "#### Step 4: Vectorization, analysis and basic plots\n",
    "#### Step 5: Printing and plotting most influential words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e2732-d47e-425a-9036-10fb75b24e8f",
   "metadata": {},
   "source": [
    "## Step 1: Imports, functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26eee37a-c9ad-4a24-a573-a54505842c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "## dependencies: spacy, en_core_web_sm, nltk stopwords\n",
    "####################################\n",
    "\n",
    "## General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Imports for NLP\n",
    "import nltk, re, spacy, string\n",
    "from spacy.lang.en.examples import sentences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "## Imports for analyses\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "\n",
    "## Function for encoding\n",
    "\n",
    "def oh_encoder(df, column):\n",
    "    \"\"\"Simple one-hot encoder that takes a dataframe and a column name and returns the dataframe with encoded column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    all_categories = list(set(g for categories in df[column] for g in categories))\n",
    "    one_hot_df = pd.DataFrame(0, index=df.index, columns=all_categories)\n",
    "    for i, categories in enumerate(df[column]):\n",
    "            one_hot_df.loc[i, categories] = 1\n",
    "    df = df.drop(columns=[column]).join(one_hot_df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## Function for data preparation\n",
    "\n",
    "def data_preparation(df):\n",
    "    \"\"\"Take a dataframe, prepare it for use in NLP and analyses and return prepared dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (Dataframe): Original dataframe to be prepared\n",
    "        \n",
    "    Returns:\n",
    "        df (Dataframe): Prepared dataframe\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    ###############################    \n",
    "    ## Data Cleaning & Recoding\n",
    "    ###############################\n",
    "    \n",
    "    ## drop columns not used in analyses\n",
    "    df.drop(['sid', 'store_url', 'store_promo_url', 'published_meta', 'published_stsp', 'published_hltb',\n",
    "           'published_igdb', 'image', 'current_price', 'discount', \n",
    "           'gfq_url', 'gfq_difficulty_comment', 'gfq_rating_comment', 'gfq_length_comment',\n",
    "           'hltb_url', 'meta_url', 'igdb_url'], axis=1, inplace=True)\n",
    "    \n",
    "    ## publish date as timedelta\n",
    "    df[\"published_store\"] = pd.to_datetime(df[\"published_store\"]) - pd.Timestamp(1997, 1, 1)\n",
    "    df[\"published_store\"] = df[\"published_store\"].apply(lambda value: value.days)\n",
    "     \n",
    "    ## missing data 1: If language or voiceover is missing, set to \"One_unknown\"\n",
    "    df.loc[df[\"languages\"].isna(), \"languages\"] = \"One_unknown\"\n",
    "    df.loc[df[\"voiceovers\"].isna(), \"voiceovers\"] = \"One_unknown\"\n",
    "\n",
    "    ## delete games without English as language:\n",
    "    count_no_en = 0\n",
    "    for x in df.index:\n",
    "        if \"english\" not in df.loc[x,\"languages\"].lower():\n",
    "            count_no_en += 1\n",
    "            df = df.drop(labels=x, axis=0)\n",
    "    print(f\"Games without English language: {count_no_en}\")\n",
    "             \n",
    "    ## use only number of languages and voiceovers\n",
    "    df[\"languages\"] = df[\"languages\"].apply(lambda value: len(value.split(\",\")))\n",
    "    df[\"voiceovers\"] = df[\"voiceovers\"].apply(lambda value: len(value.split(\",\")))\n",
    "      \n",
    "    ## missing data 2: drop columns with more than 75% missing data:\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > df.shape[0]*0.75:\n",
    "            df.drop(col, axis=1, inplace=True) \n",
    "    \n",
    "    ## One-Hot-Encoding\n",
    "    \n",
    "    ## Genres\n",
    "    ## split strings in genre and platform columns\n",
    "    df['genres'] = df['genres'].apply(lambda x: x.split(','))\n",
    "    df['platforms'] = df['platforms'].apply(lambda x: x.split(','))\n",
    "    ## replace genres\n",
    "    df['genres'] = df['genres'].apply(lambda genres: list(set(['Indie' if genre == 'Инди' else genre for genre in genres])))\n",
    "    df['genres'] = df['genres'].apply(lambda genres: list(set(['Adventure' if genre == 'Приключенческие игры' else genre for genre in genres])))\n",
    "    \n",
    "    ## One-Hot Encoding\n",
    "    df = oh_encoder(df, \"genres\")\n",
    "    df = oh_encoder(df, \"platforms\")\n",
    "    \n",
    "    ## Rename columns including spaces\n",
    "    df.rename(columns={'Game Development':'Game_Development',\n",
    "                      'Free to Play':'Free_to_Play',\n",
    "                      'Massively Multiplayer':'Massively_Multiplayer',\n",
    "                      'Early Access':'Early_Access',\n",
    "                      'Sexual Content':'Sexual_Content'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## Function for text cleaning\n",
    "\n",
    "def text_cleaner(sentence):\n",
    "    \"\"\"Take a string, clean it for use in vectorization and return cleaned string.\n",
    "    \n",
    "    Args:\n",
    "        sentence (string): Original string to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        doc_str (string): Cleaned String\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ## counter\n",
    "    global call_count \n",
    "    call_count += 1\n",
    "    if call_count%1000 == 0:\n",
    "        print(call_count)\n",
    "    if sentence is None:\n",
    "        doc_str = \"\"\n",
    "    else:\n",
    "        ## tokenize and delete pronouns, stopwords and punctuation\n",
    "        doc = nlp(sentence)\n",
    "        clean_doc = [token.lemma_.lower() for token in doc if (token.pos_ !=\"PRON\") and (token.lemma_ not in stopWords) and (token.lemma_ not in punctuations)]\n",
    "        ## rejoin texts\n",
    "        doc_str = \" \".join(clean_doc)\n",
    "        ## deleting points, tabs, spaces and line breaks\n",
    "        doc_str = re.sub(\"[\\s]+\", \" \", doc_str)\n",
    "        ## deleting numbers\n",
    "        doc_str = re.sub(r'\\d+', '', doc_str) \n",
    "    return doc_str\n",
    "\n",
    "\n",
    "## Class for analyses\n",
    "\n",
    "class NLPAnalyzer():\n",
    "    \"\"\"A class used for the analyses including NLP.\n",
    "\n",
    "        Attributes:\n",
    "\n",
    "            df (Dataframe):\n",
    "                The DataFrame to be used for analyses.\n",
    "            target_var (string):\n",
    "                The name of the target column.\n",
    "            max_feature_list (list):\n",
    "                A list of interger values to be used as maximum number of features for vectorization.\n",
    "            test_size (float):\n",
    "                The fraction of the Dataframe to be used as the test sample.\n",
    "            tfidf (bool):\n",
    "                A Boolean that is True if TF-IDF vectorization should be used instead of BOW/Count vectorization.\n",
    "            var_dict (dictionary):\n",
    "                A dictionary of potential targets with column names (keys) and column descriptions (values).\n",
    "                These columns are excluded from analyses and only the target used in target_var is included as target.\n",
    "            train_data (Dataframe):\n",
    "                Data to be used as train features. Generated when runnning :meth: \"analyze\".\n",
    "            test_data (Dataframe):\n",
    "                Data to be used as test features. Generated when runnning :meth: \"analyze\".\n",
    "            train_target (Dataframe):\n",
    "                Data to be used as train target. Generated when runnning :meth: \"analyze\".\n",
    "            test_target (Dataframe):\n",
    "                Data to be used as test target. Generated when runnning :meth: \"analyze\".\n",
    "            vectorizer (instance of vectorizer class from sklearn):\n",
    "                Vectorizer used. Can be CountVectorizer or TfidfVectorizer. Generated when runnning :meth: analyze.\n",
    "            model_sm (instance of statsmodels.api.OLS):\n",
    "                OLS regression used for extraction of t-values. Generated when runnning :meth: analyze.\n",
    "        \n",
    "        Methods:\n",
    "    \n",
    "            analyze():\n",
    "                Conduct analyses, print figures for Adjusted R-squared and T-values of non-NLP features. Return table of results.\n",
    "  \n",
    "            extract_words(max_features=50):\n",
    "                Conduct OLS regression for max_features and return list of words showing significant effects and corresponding t-values.\n",
    "                \n",
    "            plot_words_multi(word_list_1, word_list_2, target_1, target_2, top_number=25):\n",
    "                Plot words of two analyses extracted with :meth: extract_words.\n",
    "                       \n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_var, target_name, max_feature_list=[0, 2000, 2500, 3000, 3500, 4000], test_size=0.25, tfidf=False, naming_suffix=\"\"):\n",
    "        \"\"\"Construct all the necessary attributes for the NLPAnalyzer object.\n",
    "\n",
    "        Args:\n",
    "            df (Dataframe):\n",
    "                The DataFrame to be used for analyses.\n",
    "            target_var (string):\n",
    "                The target column.\n",
    "            target_name (string):\n",
    "                The name of the target column (used for plotting).\n",
    "            max_feature_list (list):\n",
    "                A list of interger values to be used as maximum number of features for vectorization.\n",
    "            test_size (float):\n",
    "                The fraction of the Dataframe to be used as the test sample.\n",
    "            tfidf (bool):\n",
    "                A Boolean that is True if TF-IDF vectorization should be used instead of BOW/Count vectorization.\n",
    "            naming_suffix (string):\n",
    "                A string that is added to plots for simple comparison of different models for a specific target.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.target_var = target_var\n",
    "        self.target_name = target_name\n",
    "        self.test_size = test_size\n",
    "        self.max_feature_list = max_feature_list\n",
    "        self.tfidf = tfidf\n",
    "        self.naming_suffix = naming_suffix\n",
    "        \n",
    "        print(\"*\"*50, \"\\n\", \"Initialization - Target:\", self.target_var, \"\\n\", \"*\"*50)\n",
    "\n",
    "        ## delete missings\n",
    "        self.df = self.df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "        ## Reset index\n",
    "        self.df.reset_index()\n",
    "        \n",
    "        print(\"*\"*50, \"\\n\", \"Data preparation done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Conduct analyses, print figures for Adjusted R-squared and T-values of non-NLP features. Return list of results.     \n",
    "            \n",
    "            Returns:\n",
    "                results_df (Dataframe):\n",
    "                    Dataframe of models used, maximum number of features and corresponding values of adjusted R-squared.\n",
    "                    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Train-test split\n",
    "        self.train_data, self.test_data, self.train_target, self.test_target = train_test_split(\n",
    "            self.df.drop([self.target_var], axis=1), self.df[self.target_var], test_size=self.test_size, random_state=42)\n",
    "        \n",
    "        # List for results\n",
    "        results = []\n",
    "\n",
    "        # Standardize numeric features\n",
    "        columns_to_scale = [column for column in self.train_data.columns if len(self.train_data[column].unique()) > 2 and \n",
    "                            np.issubdtype(self.train_data[column].dtype, np.number)]\n",
    "        print(f\"Scaling the following columns for analyses: {columns_to_scale}\")\n",
    "        scaler = StandardScaler()\n",
    "        self.train_data[columns_to_scale] = scaler.fit_transform(self.train_data[columns_to_scale])\n",
    "        self.test_data[columns_to_scale] = scaler.transform(self.test_data[columns_to_scale])\n",
    "\n",
    "        \n",
    "        # Loop for different values of max_features\n",
    "        for max_feat in self.max_feature_list:\n",
    "            try:\n",
    "                if max_feat > 0:\n",
    "\n",
    "                    if self.tfidf:\n",
    "                        self.vectorizer = TfidfVectorizer(\n",
    "                            stop_words='english',\n",
    "                            max_df=0.9,\n",
    "                            min_df=10,\n",
    "                            max_features=max_feat\n",
    "                        )\n",
    "\n",
    "                        print(\"*\"*50, \"\\n\", \"Using TF-IDF Vectorizer\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                    else:\n",
    "                        self.vectorizer = CountVectorizer(\n",
    "                            stop_words='english',\n",
    "                            max_df=0.9,\n",
    "                            min_df=10,\n",
    "                            max_features=max_feat\n",
    "                        )\n",
    "                        \n",
    "                        print(\"*\"*50, \"\\n\", \"Using BOW Vectorizer\", \"\\n\", \"*\"*50)\n",
    "                    \n",
    "                    # Fit and transformation\n",
    "                    X_train_text = self.vectorizer.fit_transform(self.train_data['description_clean_nonum'])\n",
    "                    X_test_text = self.vectorizer.transform(self.test_data['description_clean_nonum'])\n",
    "                    \n",
    "                    # Convert text features into dataframe\n",
    "                    X_train_text_df = pd.DataFrame(X_train_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.train_data.index)\n",
    "                    X_test_text_df = pd.DataFrame(X_test_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.test_data.index)\n",
    "                else:\n",
    "                    # When max_feat is 0, use only non-text features\n",
    "                    X_train_text_df = pd.DataFrame()\n",
    "                    X_test_text_df = pd.DataFrame()\n",
    "               \n",
    "                # Keep non-text features\n",
    "                X_train_non_text = self.train_data.drop('description_clean_nonum', axis=1)\n",
    "                X_test_non_text = self.test_data.drop('description_clean_nonum', axis=1)\n",
    "                \n",
    "                # Make sure all non-text features are numeric\n",
    "                X_train_non_text = X_train_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                X_test_non_text = X_test_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                \n",
    "                # Concatenate dataframes\n",
    "                X_train = pd.concat([X_train_non_text, X_train_text_df], axis=1)\n",
    "                X_test = pd.concat([X_test_non_text, X_test_text_df], axis=1)\n",
    "                \n",
    "                # Make sure all features are numeric\n",
    "                X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "                X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "       \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Vectorization done\", \"\\n\", \"*\"*50)\n",
    "                \n",
    "                \n",
    "                # OLS with statsmodels\n",
    "                self.model_sm = sm.OLS(self.train_target, sm.add_constant(X_train)).fit()\n",
    "                adj_r2_sm = self.model_sm.rsquared_adj\n",
    "                if max_feat == 0:\n",
    "                    display(self.model_sm.summary())\n",
    "                print(\"*\"*50, \"\\n\", \"R-squared for Statsmodels OLS (train data):\", self.model_sm.rsquared)\n",
    "                print(\"*\"*50, \"\\n\", \"Adjusted R-squared for Statsmodels OLS (train data):\", adj_r2_sm)\n",
    "                \n",
    "                # Add constant to X_test for prediction\n",
    "                X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "                r2_sm_test = r2_score(self.test_target, self.model_sm.predict(X_test_const))\n",
    "                adj_r2_sm_test = 1 - ( ( (1-r2_sm_test) * (len(self.test_target) - 1) ) / ( (len(self.test_target) - X_test_const.shape[1] - 1) ) )\n",
    "                print(\"*\"*50, \"\\n\", \"R-squared for Statsmodels OLS (test data):\", r2_sm_test)    \n",
    "                print(\"*\"*50, \"\\n\", \"Adjusted R-squared for Statsmodels OLS (test data):\", adj_r2_sm_test)                          \n",
    "                results.append(('OLS', max_feat, adj_r2_sm_test))\n",
    "              \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - OLS SM done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # OLS with sklearn\n",
    "                model_lr = LinearRegression(fit_intercept=True).fit(X_train, self.train_target)\n",
    "                r2_lr = r2_score(self.test_target, model_lr.predict(X_test))\n",
    "                adj_r2_lr = 1 - (1-r2_lr)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('LinearRegression', max_feat, adj_r2_lr))\n",
    "                print(r2_lr, adj_r2_lr)\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - OLS SK done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # Lasso\n",
    "                model_lasso = Lasso(fit_intercept=True, alpha=1e4).fit(X_train, self.train_target)\n",
    "                r2_lasso = r2_score(self.test_target, model_lasso.predict(X_test))\n",
    "                adj_r2_lasso = 1 - (1-r2_lasso)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('Lasso', max_feat, adj_r2_lasso))\n",
    "                print(r2_lasso, adj_r2_lasso)\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Lasso done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # Ridge\n",
    "                model_ridge = Ridge(fit_intercept=True, alpha=1e4).fit(X_train, self.train_target)\n",
    "                r2_ridge = r2_score(self.test_target, model_ridge.predict(X_test))\n",
    "                adj_r2_ridge = 1 - (1-r2_ridge)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('Ridge', max_feat, adj_r2_ridge))\n",
    "                print(r2_ridge, adj_r2_ridge)\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Ridge done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "        \n",
    "                # ElasticNet\n",
    "                model_ela = ElasticNet(fit_intercept=True, alpha=1).fit(X_train, self.train_target)\n",
    "                r2_ela = r2_score(self.test_target, model_ela.predict(X_test))\n",
    "                adj_r2_ela = 1 - (1-r2_ela)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('ElasticNet', max_feat, adj_r2_ela))\n",
    "                print(r2_ela, adj_r2_ela)\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - ElasticNet done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "                \n",
    "                # RandomForestRegressor\n",
    "                model_rf = RandomForestRegressor(n_estimators=200).fit(X_train, self.train_target)\n",
    "                r2_rf = r2_score(self.test_target, model_rf.predict(X_test))\n",
    "                adj_r2_rf = 1 - (1-r2_rf)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('RandomForest', max_feat, adj_r2_rf))\n",
    "                print(r2_rf, adj_r2_rf)\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - RF done\", \"\\n\", \"*\"*50)\n",
    "                \n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"Error with max_features={max_feat}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Convert results into dataframe\n",
    "        self.results_df = pd.DataFrame(results, columns=['Model', 'Max_Features', 'Adjusted_R2'])\n",
    "        self.results_df.loc[self.results_df[\"Adjusted_R2\"]<0,\"Adjusted_R2\"] = 0.0\n",
    "        \n",
    "        # Plot adjusted R-squared for different max features\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        for model in self.results_df['Model'].unique()[1:]:\n",
    "            subset = self.results_df[self.results_df['Model'] == model]\n",
    "            plt.plot(subset['Max_Features'], subset['Adjusted_R2'], label=model, alpha=0.5)\n",
    "    \n",
    "        plt.xlabel('Max Features')\n",
    "        plt.ylabel('Adjusted R-squared')\n",
    "        plt.title(f'Adjusted R-squared for Different Models (predicting {self.target_name})')\n",
    "        plt.xticks(self.max_feature_list)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'../../plots/fig_{self.target_var}_R2{self.naming_suffix}.png')\n",
    "    \n",
    "        # Plot t-values for non-text variables as horizontal bar plot\n",
    "        # Sorting\n",
    "        non_text_t_values = self.model_sm.tvalues[1:len(X_train_non_text.columns) + 1]\n",
    "        non_text_feature_names = X_train_non_text.columns\n",
    "        df_t_values = pd.DataFrame({\n",
    "            'feature': non_text_feature_names,\n",
    "            't_value': non_text_t_values\n",
    "        })\n",
    "        df_t_values_sorted = df_t_values.sort_values(by='t_value', ascending=False)\n",
    "        non_text_t_values = df_t_values_sorted['t_value']\n",
    "        non_text_feature_names = df_t_values_sorted['feature']\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.barh(non_text_feature_names, non_text_t_values, color=\"cornflowerblue\")\n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Non-text Features')\n",
    "        plt.title(f'T-values for Non-text Variables (predicting {self.target_name})')\n",
    "\n",
    "        max_y=len(non_text_feature_names)\n",
    "        min_x= min(non_text_t_values)-0.5\n",
    "        max_x= max(non_text_t_values)+0.5\n",
    "        l1=plt.axvline(1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        l2=plt.axvline(-1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        plt.axvline(0, color=\"cornflowerblue\", alpha=0.5)\n",
    "        f1=plt.gca().fill_between(x=[-1.96, 1.96], y1=-1, y2=max_y, color=\"red\", alpha=0.075)\n",
    "        f2=plt.gca().fill_between(x=[min_x,-1.96], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        f3=plt.gca().fill_between(x=[1.96, max_x], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        \n",
    "        legend = plt.gca().get_legend()\n",
    "        if legend:\n",
    "            legend.remove()        \n",
    "        ax2=plt.gca().twinx()\n",
    "        ax2.legend(handles=[f2], loc=1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../../plots/fig_{self.target_var}_Tvalues{self.naming_suffix}.png')\n",
    "    \n",
    "        return self.results_df\n",
    "\n",
    "    \n",
    "    def extract_words(self, max_features=50):\n",
    "        \"\"\"Conduct OLS regression for max_features and return list of significant words and corresponding t-values.\n",
    "                \n",
    "            Args:      \n",
    "                max_features (int):\n",
    "                    maximum number of features to be used for extraction of words showing significant effects.\n",
    "                        \n",
    "            Returns:\n",
    "                significant_words (list):\n",
    "                    List of words showing significant effects and corresponding t-values.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        max_feat = max_features\n",
    "        \n",
    "        # OLS for t-value extraction with specific number of words\n",
    "        try:\n",
    "            if max_features > 0:\n",
    "\n",
    "                if self.tfidf:\n",
    "                    self.vectorizer = TfidfVectorizer(\n",
    "                        stop_words='english',\n",
    "                        max_df=0.9,\n",
    "                        min_df=5,\n",
    "                        max_features=max_feat\n",
    "                    )\n",
    "\n",
    "                    print(\"*\"*50, \"\\n\", \"Using TF-IDF Vectorizer\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                else:\n",
    "                    self.vectorizer = CountVectorizer(\n",
    "                        stop_words='english',\n",
    "                        max_df=0.9,\n",
    "                        min_df=5,\n",
    "                        max_features=max_feat\n",
    "                    )\n",
    "                        \n",
    "                    print(\"*\"*50, \"\\n\", \"Using BOW Vectorizer\", \"\\n\", \"*\"*50)\n",
    "                    \n",
    "                # Fit and transformation\n",
    "                X_train_text = self.vectorizer.fit_transform(self.train_data['description_clean_nonum'])\n",
    "                X_test_text = self.vectorizer.transform(self.test_data['description_clean_nonum'])\n",
    "                    \n",
    "                # Convert text features into dataframe\n",
    "                X_train_text_df = pd.DataFrame(X_train_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.train_data.index)\n",
    "                X_test_text_df = pd.DataFrame(X_test_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.test_data.index)\n",
    "            \n",
    "            else:\n",
    "                # When max_feat is 0, word extraction is not conducted\n",
    "                print(\"There are no influential words if maximum number of text features is zero.\")\n",
    "                return self\n",
    "               \n",
    "            # Keep non-text features\n",
    "            X_train_non_text = self.train_data.drop('description_clean_nonum', axis=1)\n",
    "            X_test_non_text = self.test_data.drop('description_clean_nonum', axis=1)\n",
    "                \n",
    "            # Make sure all non-text features are numeric\n",
    "            X_train_non_text = X_train_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "            X_test_non_text = X_test_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                \n",
    "            # Concatenate dataframes\n",
    "            X_train = pd.concat([X_train_non_text, X_train_text_df], axis=1)\n",
    "            X_test = pd.concat([X_test_non_text, X_test_text_df], axis=1)\n",
    "                \n",
    "            # Make sure all features are numeric\n",
    "            X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "            X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "      \n",
    "            print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Vectorization done\", \"\\n\", \"*\"*50)\n",
    "              \n",
    "            # OLS with statsmodels\n",
    "            self.model_sm = sm.OLS(self.train_target, sm.add_constant(X_train)).fit()\n",
    "                         \n",
    "        except ValueError as e:\n",
    "            print(f\"Error with max_features={max_feat}: {e}\")\n",
    "      \n",
    "        # Extract significant words\n",
    "        significant_words = []\n",
    "        text_feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        for i, feature_name in enumerate(text_feature_names):\n",
    "            t_value = self.model_sm.tvalues[len(X_train_non_text.columns) + i + 1]\n",
    "            \n",
    "            if abs(t_value) > 1.96:\n",
    "                significant_words.append((feature_name, t_value))\n",
    "        \n",
    "        return significant_words\n",
    "\n",
    "\n",
    "    def plot_words(self, number_words=50, max_words=25):\n",
    "        \"\"\"Plot words with significant influence on target variable in OLS regression.\n",
    "\n",
    "        Args:\n",
    "            number_words (int):\n",
    "                Number of words to be used in OLS regression for extraction of words.\n",
    "            max_words (int):\n",
    "                Maximum number of words to be plotted.\n",
    "\n",
    "        Returns:\n",
    "            top (list):\n",
    "                List of top words with highest t-values. Length is based on max_words.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Sort word lists based on t-values in descending order\n",
    "        sorted_words = sorted(self.extract_words(number_words), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top words\n",
    "        top = sorted_words[:max_words]\n",
    "\n",
    "        # Extract words and t-values\n",
    "        words = [entry[0] for entry in top]\n",
    "        t_values = [entry[1] for entry in top]\n",
    "         \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "               \n",
    "        # Make barh plot\n",
    "        w1=plt.barh(words, t_values, color=\"cornflowerblue\", label=self.target_name, alpha=0.5)\n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Words')\n",
    "        plt.title(f'T-values of most influential words in models for {self.target_name}')\n",
    "        \n",
    "        max_y=len(words)\n",
    "        min_x= min(t_values)-0.5\n",
    "        max_x= max(t_values)+0.5\n",
    "        l1=plt.axvline(1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        l2=plt.axvline(-1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        plt.axvline(0, color=\"cornflowerblue\", alpha=0.5)\n",
    "        f1=plt.gca().fill_between(x=[-1.96, 1.96], y1=-1, y2=max_y, color=\"red\", alpha=0.075)\n",
    "        f2=plt.gca().fill_between(x=[min_x,-1.96], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        f3=plt.gca().fill_between(x=[1.96, max_x], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        \n",
    "        legend = plt.gca().get_legend()\n",
    "        if legend:\n",
    "            legend.remove()\n",
    "        ax2=plt.gca().twinx()\n",
    "        ax2.legend(handles=[f2], loc=1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../../plots/fig_word_t-values_{self.target_var}{self.naming_suffix}.png')\n",
    "\n",
    "        return top\n",
    "\n",
    "\n",
    "    def plot_words_multi(self, word_list_1, word_list_2, target_1, target_2, max_words=25):\n",
    "        \"\"\"Plot words of two analyses extracted with :meth: extract_words.\n",
    "\n",
    "        Args:\n",
    "            words_list_1 (list):\n",
    "                List of words and corresponding t-values extracted with :meth: extract_words. \n",
    "            words_list_1 (list):\n",
    "                List of words and corresponding t-values extracted with :meth: extract_words. \n",
    "            target_1 (string):\n",
    "                Name of dependent variable predicted with word_list_1. \n",
    "            target_2 (string):\n",
    "                Name of dependent variable predicted with word_list_2. \n",
    "            max_words (int):\n",
    "                Maximum number of words to be extracted and plotted.\n",
    "\n",
    "            Returns:\n",
    "                top_1 (list):\n",
    "                    List of top words with highest t-values from word_list_1. Length is based on :attr: max_words.\n",
    "                top_2 (list):\n",
    "                    List of top words with highest t-values from word_list_1. Length is based on :attr: max_words.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Sort word lists based on t-values in descending order\n",
    "        sorted_words_1 = sorted(word_list_1, key=lambda x: x[1], reverse=True)\n",
    "        sorted_words_2 = sorted(word_list_2, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top words\n",
    "        top_1 = sorted_words_1[:max_words]\n",
    "        top_2 = sorted_words_2[:max_words]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Extract words and t-values\n",
    "        words_1 = [entry[0] for entry in top_1]\n",
    "        t_values_1 = [entry[1] for entry in top_1]\n",
    "        words_2 = [entry[0] for entry in top_2]\n",
    "        t_values_2 = [entry[1] for entry in top_2]\n",
    "        \n",
    "        # Make barh plot\n",
    "        w1=plt.barh(words_1, t_values_1, color='orange', label=target_1, alpha=0.5)\n",
    "        w2=plt.barh(words_2, t_values_2, color=\"cornflowerblue\", label=target_2, alpha=0.5)\n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Words')\n",
    "        plt.title(f'T-values of most influential words in models for {target_1} and {target_2}')\n",
    "\n",
    "        unique_values = list(set(words_1) | set(words_2))\n",
    "        max_y=len(unique_values)\n",
    "        min_x= min(min(t_values_1), max(t_values_1))-0.5\n",
    "        max_x= max(max(t_values_1), max(t_values_1))+0.5\n",
    "        l1=plt.axvline(1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        l2=plt.axvline(-1.96, color=\"green\", label=\"Significant with p<0.05\", alpha=0.75)\n",
    "        plt.axvline(0, color=\"cornflowerblue\", alpha=0.5)\n",
    "        f1=plt.gca().fill_between(x=[-1.96, 1.96], y1=-1, y2=max_y, color=\"red\", alpha=0.075)\n",
    "        f2=plt.gca().fill_between(x=[min_x,-1.96], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        f3=plt.gca().fill_between(x=[1.96, max_x], y1=-1, y2=max_y, color=\"green\", alpha=0.075)\n",
    "        plt.gca().legend(handles=[w1, w2], loc=2, title=\"Models\")\n",
    "        ax2=plt.gca().twinx()\n",
    "        ax2.legend(handles=[f2], loc=1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../../plots/fig_word_t-values_{target_1.replace(\" \", \"\")}_{target_2.replace(\" \", \"\")}{self.naming_suffix}.png')\n",
    "\n",
    "        return top_1, top_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7eccd-056a-4e97-b680-fff01ae036a1",
   "metadata": {},
   "source": [
    "## Step 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5bda8f-6c55-4787-b69f-caa128a4fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games without English language: 1931\n"
     ]
    }
   ],
   "source": [
    "## Read data\n",
    "df = pd.read_json(\"../../data/steamdb.json\")\n",
    "\n",
    "## Prepare data\n",
    "df = data_preparation(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e8eda-0f66-4bff-b478-da4292ebdde5",
   "metadata": {},
   "source": [
    "## Step 3: NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432eb81e-4c35-47ff-94ad-0f0125d176aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Starting text cleaner\n",
      "**************************************************\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#########################################\n",
    "## Users might need to manually download stopwords:\n",
    "# nltk.download('stopwords')\n",
    "#########################################\n",
    "\n",
    "## Clean \"description\"\n",
    "stopWords = stopwords.words(\"english\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "## Applying text_cleaner to description\n",
    "print(\"*\"*50 + \"\\nStarting text cleaner\\n\" + \"*\"*50) \n",
    "## Adding a global counter to print in text_cleaner function since it takes a long time\n",
    "call_count = 0\n",
    "## Using text_cleaner\n",
    "df[\"description_clean_nonum\"] = df[\"description\"].apply(text_cleaner)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3268e2-8e2a-41d1-9a50-3243f20ddb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## optional: save as pickle for fast reload\n",
    "# df.to_pickle(\"../../data/df_with_lemmas_compressed.pkl\", compression='bz2')\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14de48-3808-4e5d-8567-8766dc2033c5",
   "metadata": {},
   "source": [
    "## Step 4: Vectorization, analysis and basic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e91641-7d1d-44f4-a5c2-0d02b8f39828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## optional: if saved as pickle, load it here:\n",
    "# df = pd.read_pickle(\"../../data/df_with_lemmas_compressed.pkl\", compression='bz2')\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5eb2f-a821-4bc3-bb2a-268a82c770ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " Initialization - Target: stsp_owners \n",
      " **************************************************\n",
      "************************************************** \n",
      " Data preparation done \n",
      " **************************************************\n",
      "Scaling the following columns for analyses: ['published_store', 'full_price', 'languages', 'voiceovers']\n",
      "************************************************** \n",
      " NLP: 0 words - Vectorization done \n",
      " **************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>stsp_owners</td>   <th>  R-squared:         </th>  <td>   0.053</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.052</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   53.09</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 12 Jun 2024</td> <th>  Prob (F-statistic):</th>  <td>6.74e-247</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:22:14</td>     <th>  Log-Likelihood:    </th> <td>-3.3672e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22572</td>      <th>  AIC:               </th>  <td>6.735e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22547</td>      <th>  BIC:               </th>  <td>6.737e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                 <td> 1.133e+04</td> <td> 4.21e+05</td> <td>    0.027</td> <td> 0.979</td> <td>-8.14e+05</td> <td> 8.37e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>published_store</th>       <td>-1.262e+05</td> <td> 5152.892</td> <td>  -24.483</td> <td> 0.000</td> <td>-1.36e+05</td> <td>-1.16e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>full_price</th>            <td> 5.538e+04</td> <td> 4889.805</td> <td>   11.325</td> <td> 0.000</td> <td> 4.58e+04</td> <td>  6.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>languages</th>             <td> 8.909e+04</td> <td> 5881.568</td> <td>   15.148</td> <td> 0.000</td> <td> 7.76e+04</td> <td> 1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>voiceovers</th>            <td>-1.489e+04</td> <td> 5852.944</td> <td>   -2.545</td> <td> 0.011</td> <td>-2.64e+04</td> <td>-3422.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Indie</th>                 <td>-2.367e+04</td> <td> 1.15e+04</td> <td>   -2.063</td> <td> 0.039</td> <td>-4.62e+04</td> <td>-1179.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Casual</th>                <td>  -2.1e+04</td> <td> 1.04e+04</td> <td>   -2.020</td> <td> 0.043</td> <td>-4.14e+04</td> <td> -625.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Massively_Multiplayer</th> <td>-2.237e+04</td> <td> 3.09e+04</td> <td>   -0.723</td> <td> 0.470</td> <td> -8.3e+04</td> <td> 3.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Movie</th>                 <td>-8.022e-11</td> <td> 2.43e-10</td> <td>   -0.331</td> <td> 0.741</td> <td>-5.56e-10</td> <td> 3.96e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gore</th>                  <td>-2.457e+04</td> <td> 6.09e+04</td> <td>   -0.403</td> <td> 0.687</td> <td>-1.44e+05</td> <td> 9.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Early_Access</th>          <td> 3692.9254</td> <td> 1.57e+04</td> <td>    0.236</td> <td> 0.814</td> <td> -2.7e+04</td> <td> 3.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Adventure</th>             <td>  312.6024</td> <td> 1.05e+04</td> <td>    0.030</td> <td> 0.976</td> <td>-2.02e+04</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Nudity</th>                <td>-2.902e+04</td> <td> 8.79e+04</td> <td>   -0.330</td> <td> 0.741</td> <td>-2.01e+05</td> <td> 1.43e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Violent</th>               <td>-1.917e+04</td> <td> 4.66e+04</td> <td>   -0.412</td> <td> 0.681</td> <td> -1.1e+05</td> <td> 7.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sexual_Content</th>        <td> 1140.7100</td> <td> 8.93e+04</td> <td>    0.013</td> <td> 0.990</td> <td>-1.74e+05</td> <td> 1.76e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Racing</th>                <td>-3.423e+04</td> <td> 2.61e+04</td> <td>   -1.309</td> <td> 0.190</td> <td>-8.55e+04</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Game_Development</th>      <td> 3.644e+04</td> <td> 2.58e+05</td> <td>    0.141</td> <td> 0.888</td> <td>-4.69e+05</td> <td> 5.42e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sports</th>                <td>-1.457e+04</td> <td> 2.34e+04</td> <td>   -0.623</td> <td> 0.533</td> <td>-6.04e+04</td> <td> 3.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Simulation</th>            <td> 1.381e+04</td> <td> 1.29e+04</td> <td>    1.071</td> <td> 0.284</td> <td>-1.15e+04</td> <td> 3.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Action</th>                <td> 2.002e+04</td> <td> 1.03e+04</td> <td>    1.943</td> <td> 0.052</td> <td> -175.097</td> <td> 4.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Free_to_Play</th>          <td>-1.707e+04</td> <td> 2.04e+04</td> <td>   -0.838</td> <td> 0.402</td> <td> -5.7e+04</td> <td> 2.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RPG</th>                   <td> 2536.0856</td> <td> 1.37e+04</td> <td>    0.185</td> <td> 0.854</td> <td>-2.44e+04</td> <td> 2.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Strategy</th>              <td>-1.162e+04</td> <td> 1.27e+04</td> <td>   -0.913</td> <td> 0.362</td> <td>-3.66e+04</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MAC</th>                   <td>  1.73e+04</td> <td> 1.37e+04</td> <td>    1.261</td> <td> 0.207</td> <td>-9596.598</td> <td> 4.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LNX</th>                   <td> 4.872e+04</td> <td> 1.59e+04</td> <td>    3.071</td> <td> 0.002</td> <td> 1.76e+04</td> <td> 7.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WIN</th>                   <td> 9.103e+04</td> <td> 4.21e+05</td> <td>    0.216</td> <td> 0.829</td> <td>-7.35e+05</td> <td> 9.17e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>74497.158</td> <th>  Durbin-Watson:     </th>    <td>   2.005</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>25108254236.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>56.976</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>5168.629</td>  <th>  Cond. No.          </th>    <td>1.22e+16</td>    \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.06e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &   stsp\\_owners   & \\textbf{  R-squared:         } &        0.053     \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &        0.052     \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &        53.09     \\\\\n",
       "\\textbf{Date:}                  & Wed, 12 Jun 2024 & \\textbf{  Prob (F-statistic):} &    6.74e-247     \\\\\n",
       "\\textbf{Time:}                  &     22:22:14     & \\textbf{  Log-Likelihood:    } &   -3.3672e+05    \\\\\n",
       "\\textbf{No. Observations:}      &       22572      & \\textbf{  AIC:               } &    6.735e+05     \\\\\n",
       "\\textbf{Df Residuals:}          &       22547      & \\textbf{  BIC:               } &    6.737e+05     \\\\\n",
       "\\textbf{Df Model:}              &          24      & \\textbf{                     } &                  \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &                  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}                  &    1.133e+04  &     4.21e+05     &     0.027  &         0.979        &    -8.14e+05    &     8.37e+05     \\\\\n",
       "\\textbf{published\\_store}       &   -1.262e+05  &     5152.892     &   -24.483  &         0.000        &    -1.36e+05    &    -1.16e+05     \\\\\n",
       "\\textbf{full\\_price}            &    5.538e+04  &     4889.805     &    11.325  &         0.000        &     4.58e+04    &      6.5e+04     \\\\\n",
       "\\textbf{languages}              &    8.909e+04  &     5881.568     &    15.148  &         0.000        &     7.76e+04    &     1.01e+05     \\\\\n",
       "\\textbf{voiceovers}             &   -1.489e+04  &     5852.944     &    -2.545  &         0.011        &    -2.64e+04    &    -3422.288     \\\\\n",
       "\\textbf{Indie}                  &   -2.367e+04  &     1.15e+04     &    -2.063  &         0.039        &    -4.62e+04    &    -1179.519     \\\\\n",
       "\\textbf{Casual}                 &     -2.1e+04  &     1.04e+04     &    -2.020  &         0.043        &    -4.14e+04    &     -625.247     \\\\\n",
       "\\textbf{Massively\\_Multiplayer} &   -2.237e+04  &     3.09e+04     &    -0.723  &         0.470        &     -8.3e+04    &     3.83e+04     \\\\\n",
       "\\textbf{Movie}                  &   -8.022e-11  &     2.43e-10     &    -0.331  &         0.741        &    -5.56e-10    &     3.96e-10     \\\\\n",
       "\\textbf{Gore}                   &   -2.457e+04  &     6.09e+04     &    -0.403  &         0.687        &    -1.44e+05    &     9.48e+04     \\\\\n",
       "\\textbf{Early\\_Access}          &    3692.9254  &     1.57e+04     &     0.236  &         0.814        &     -2.7e+04    &     3.44e+04     \\\\\n",
       "\\textbf{Adventure}              &     312.6024  &     1.05e+04     &     0.030  &         0.976        &    -2.02e+04    &     2.08e+04     \\\\\n",
       "\\textbf{Nudity}                 &   -2.902e+04  &     8.79e+04     &    -0.330  &         0.741        &    -2.01e+05    &     1.43e+05     \\\\\n",
       "\\textbf{Violent}                &   -1.917e+04  &     4.66e+04     &    -0.412  &         0.681        &     -1.1e+05    &     7.21e+04     \\\\\n",
       "\\textbf{Sexual\\_Content}        &    1140.7100  &     8.93e+04     &     0.013  &         0.990        &    -1.74e+05    &     1.76e+05     \\\\\n",
       "\\textbf{Racing}                 &   -3.423e+04  &     2.61e+04     &    -1.309  &         0.190        &    -8.55e+04    &      1.7e+04     \\\\\n",
       "\\textbf{Game\\_Development}      &    3.644e+04  &     2.58e+05     &     0.141  &         0.888        &    -4.69e+05    &     5.42e+05     \\\\\n",
       "\\textbf{Sports}                 &   -1.457e+04  &     2.34e+04     &    -0.623  &         0.533        &    -6.04e+04    &     3.13e+04     \\\\\n",
       "\\textbf{Simulation}             &    1.381e+04  &     1.29e+04     &     1.071  &         0.284        &    -1.15e+04    &     3.91e+04     \\\\\n",
       "\\textbf{Action}                 &    2.002e+04  &     1.03e+04     &     1.943  &         0.052        &     -175.097    &     4.02e+04     \\\\\n",
       "\\textbf{Free\\_to\\_Play}         &   -1.707e+04  &     2.04e+04     &    -0.838  &         0.402        &     -5.7e+04    &     2.29e+04     \\\\\n",
       "\\textbf{RPG}                    &    2536.0856  &     1.37e+04     &     0.185  &         0.854        &    -2.44e+04    &     2.95e+04     \\\\\n",
       "\\textbf{Strategy}               &   -1.162e+04  &     1.27e+04     &    -0.913  &         0.362        &    -3.66e+04    &     1.33e+04     \\\\\n",
       "\\textbf{MAC}                    &     1.73e+04  &     1.37e+04     &     1.261  &         0.207        &    -9596.598    &     4.42e+04     \\\\\n",
       "\\textbf{LNX}                    &    4.872e+04  &     1.59e+04     &     3.071  &         0.002        &     1.76e+04    &     7.98e+04     \\\\\n",
       "\\textbf{WIN}                    &    9.103e+04  &     4.21e+05     &     0.216  &         0.829        &    -7.35e+05    &     9.17e+05     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 74497.158 & \\textbf{  Durbin-Watson:     } &        2.005     \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 25108254236.817  \\\\\n",
       "\\textbf{Skew:}          &   56.976  & \\textbf{  Prob(JB):          } &         0.00     \\\\\n",
       "\\textbf{Kurtosis:}      &  5168.629 & \\textbf{  Cond. No.          } &     1.22e+16     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 5.06e-28. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            stsp_owners   R-squared:                       0.053\n",
       "Model:                            OLS   Adj. R-squared:                  0.052\n",
       "Method:                 Least Squares   F-statistic:                     53.09\n",
       "Date:                Wed, 12 Jun 2024   Prob (F-statistic):          6.74e-247\n",
       "Time:                        22:22:14   Log-Likelihood:            -3.3672e+05\n",
       "No. Observations:               22572   AIC:                         6.735e+05\n",
       "Df Residuals:                   22547   BIC:                         6.737e+05\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "const                  1.133e+04   4.21e+05      0.027      0.979   -8.14e+05    8.37e+05\n",
       "published_store       -1.262e+05   5152.892    -24.483      0.000   -1.36e+05   -1.16e+05\n",
       "full_price             5.538e+04   4889.805     11.325      0.000    4.58e+04     6.5e+04\n",
       "languages              8.909e+04   5881.568     15.148      0.000    7.76e+04    1.01e+05\n",
       "voiceovers            -1.489e+04   5852.944     -2.545      0.011   -2.64e+04   -3422.288\n",
       "Indie                 -2.367e+04   1.15e+04     -2.063      0.039   -4.62e+04   -1179.519\n",
       "Casual                  -2.1e+04   1.04e+04     -2.020      0.043   -4.14e+04    -625.247\n",
       "Massively_Multiplayer -2.237e+04   3.09e+04     -0.723      0.470    -8.3e+04    3.83e+04\n",
       "Movie                 -8.022e-11   2.43e-10     -0.331      0.741   -5.56e-10    3.96e-10\n",
       "Gore                  -2.457e+04   6.09e+04     -0.403      0.687   -1.44e+05    9.48e+04\n",
       "Early_Access           3692.9254   1.57e+04      0.236      0.814    -2.7e+04    3.44e+04\n",
       "Adventure               312.6024   1.05e+04      0.030      0.976   -2.02e+04    2.08e+04\n",
       "Nudity                -2.902e+04   8.79e+04     -0.330      0.741   -2.01e+05    1.43e+05\n",
       "Violent               -1.917e+04   4.66e+04     -0.412      0.681    -1.1e+05    7.21e+04\n",
       "Sexual_Content         1140.7100   8.93e+04      0.013      0.990   -1.74e+05    1.76e+05\n",
       "Racing                -3.423e+04   2.61e+04     -1.309      0.190   -8.55e+04     1.7e+04\n",
       "Game_Development       3.644e+04   2.58e+05      0.141      0.888   -4.69e+05    5.42e+05\n",
       "Sports                -1.457e+04   2.34e+04     -0.623      0.533   -6.04e+04    3.13e+04\n",
       "Simulation             1.381e+04   1.29e+04      1.071      0.284   -1.15e+04    3.91e+04\n",
       "Action                 2.002e+04   1.03e+04      1.943      0.052    -175.097    4.02e+04\n",
       "Free_to_Play          -1.707e+04   2.04e+04     -0.838      0.402    -5.7e+04    2.29e+04\n",
       "RPG                    2536.0856   1.37e+04      0.185      0.854   -2.44e+04    2.95e+04\n",
       "Strategy              -1.162e+04   1.27e+04     -0.913      0.362   -3.66e+04    1.33e+04\n",
       "MAC                     1.73e+04   1.37e+04      1.261      0.207   -9596.598    4.42e+04\n",
       "LNX                    4.872e+04   1.59e+04      3.071      0.002    1.76e+04    7.98e+04\n",
       "WIN                    9.103e+04   4.21e+05      0.216      0.829   -7.35e+05    9.17e+05\n",
       "==============================================================================\n",
       "Omnibus:                    74497.158   Durbin-Watson:                   2.005\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):      25108254236.817\n",
       "Skew:                          56.976   Prob(JB):                         0.00\n",
       "Kurtosis:                    5168.629   Cond. No.                     1.22e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.06e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " R-squared for Statsmodels OLS (train data): 0.05349230951199968\n",
      "************************************************** \n",
      " Adjusted R-squared for Statsmodels OLS (train data): 0.052484805871971574\n",
      "************************************************** \n",
      " R-squared for Statsmodels OLS (test data): 0.0697765431937265\n",
      "************************************************** \n",
      " Adjusted R-squared for Statsmodels OLS (test data): 0.06655090837417954\n",
      "************************************************** \n",
      " NLP: 0 words - OLS SM done \n",
      " **************************************************\n",
      "0.0697765431937265 0.06667538484992641\n",
      "************************************************** \n",
      " NLP: 0 words - OLS SK done \n",
      " **************************************************\n",
      "0.06712329908270631 0.06401329541249268\n",
      "************************************************** \n",
      " NLP: 0 words - Lasso done \n",
      " **************************************************\n",
      "0.061732235110549594 0.05860425883074749\n",
      "************************************************** \n",
      " NLP: 0 words - Ridge done \n",
      " **************************************************\n",
      "0.06050427171088568 0.05737220167391699\n",
      "************************************************** \n",
      " NLP: 0 words - ElasticNet done \n",
      " **************************************************\n",
      "0.13545005270851262 0.13256783525521387\n",
      "************************************************** \n",
      " NLP: 0 words - RF done \n",
      " **************************************************\n",
      "************************************************** \n",
      " Using BOW Vectorizer \n",
      " **************************************************\n",
      "************************************************** \n",
      " NLP: 25 words - Vectorization done \n",
      " **************************************************\n",
      "************************************************** \n",
      " R-squared for Statsmodels OLS (train data): 0.058846146787017384\n",
      "************************************************** \n",
      " Adjusted R-squared for Statsmodels OLS (train data): 0.05679852495914084\n",
      "************************************************** \n",
      " R-squared for Statsmodels OLS (test data): 0.07818990903596412\n",
      "************************************************** \n",
      " Adjusted R-squared for Statsmodels OLS (test data): 0.07189895297559135\n",
      "************************************************** \n",
      " NLP: 25 words - OLS SM done \n",
      " **************************************************\n",
      "0.07818990903596412 0.07202313026312468\n",
      "************************************************** \n",
      " NLP: 25 words - OLS SK done \n",
      " **************************************************\n",
      "0.07380743199777795 0.06761133507509787\n",
      "************************************************** \n",
      " NLP: 25 words - Lasso done \n",
      " **************************************************\n",
      "0.07137362123115809 0.06516124245962451\n",
      "************************************************** \n",
      " NLP: 25 words - Ridge done \n",
      " **************************************************\n",
      "0.0702780359874623 0.06405832790602983\n",
      "************************************************** \n",
      " NLP: 25 words - ElasticNet done \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Use NLPAnalyzer class to conduct basic analyses and save basic plots of results\n",
    "        \n",
    "NLP_owners = NLPAnalyzer(df.drop([\"name\", \"hltb_single\", \"developers\", \"publishers\", \"categories\", \n",
    "                                  \"tags\", \"achievements\", \"gfq_rating\", \"description\", \"store_uscore\", \n",
    "                                  \"igdb_popularity\"], axis=1), \n",
    "                         \"stsp_owners\", \"Owners\",\n",
    "                         [0, 25, 50, 100, 150, 200, 250, 500]) \n",
    "results_owners = NLP_owners.analyze()\n",
    "\n",
    "NLP_uscore = NLPAnalyzer(df.drop([\"name\", \"hltb_single\", \"developers\", \"publishers\", \"categories\", \n",
    "                                  \"tags\", \"achievements\", \"gfq_rating\", \"description\",\n",
    "                                  \"stsp_owners\", \"igdb_popularity\"], axis=1), \n",
    "                         \"store_uscore\", \"User Score\",\n",
    "                         [0, 25, 50, 100, 150, 200, 250, 500])\n",
    "results_uscore = NLP_uscore.analyze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857512d-155b-4228-bddb-d220244dd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Analyses including hltb_single as feature\n",
    "        \n",
    "NLP_owners_with_hltb_single = NLPAnalyzer(df.drop([\"name\", \"developers\", \"publishers\", \"categories\", \n",
    "                                  \"tags\", \"achievements\", \"gfq_rating\", \"description\", \"store_uscore\", \n",
    "                                  \"igdb_popularity\"], axis=1), \n",
    "                         \"stsp_owners\", \"Owners\",\n",
    "                         [0, 25, 50, 100, 150, 200, 250],\n",
    "                        naming_suffix=\"_with_hltb_single\")\n",
    "results_owners_with_hltb_single = NLP_owners_with_hltb_single.analyze()\n",
    "\n",
    "NLP_uscore_with_hltb_single = NLPAnalyzer(df.drop([\"name\", \"developers\", \"publishers\", \"categories\", \n",
    "                                  \"tags\", \"achievements\", \"gfq_rating\", \"description\",\n",
    "                                  \"stsp_owners\", \"igdb_popularity\"], axis=1), \n",
    "                         \"store_uscore\", \"User Score\",\n",
    "                         [0, 25, 50, 100, 150, 200, 250],\n",
    "                        naming_suffix=\"_with_hltb_single\")\n",
    "results_uscore_with_hltb_single = NLP_uscore_with_hltb_single.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224c859-b749-4500-99a2-9160a8656e95",
   "metadata": {},
   "source": [
    "## Step 5: Printing and plotting most influential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d61a01-9fd9-43c7-834e-16381c901b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Extract words using NLPAnalyzer:\n",
    "words_owners = NLP_owners.extract_words(50)\n",
    "words_uscore= NLP_uscore.extract_words(50)\n",
    "        \n",
    "# Printing significant words and corresponding t-values\n",
    "for word_list in [words_owners, words_uscore]:\n",
    "    for word, tv1 in word_list:\n",
    "        print(f\"Word: {word}, t-values: {tv1}\")\n",
    "\n",
    "## Plotting most influential words using NLPAnalyzer:\n",
    "## Single plot for prediction of owners\n",
    "NLP_owners.plot_words()\n",
    "## Single plot for prediction of owners\n",
    "NLP_uscore.plot_words()\n",
    "## Combined plot for prediction of owners and user score\n",
    "NLP_owners.plot_words_multi(words_owners, words_uscore, \"Owners\", \"User Score\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef544b-9658-440b-a9c0-d1220f1483e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Extract words using NLPAnalyzer - for analyses including hltb_single as feature:\n",
    "words_owners_with_hltb_single = NLP_owners_with_hltb_single.extract_words(50)\n",
    "words_uscore_with_hltb_single= NLP_uscore_with_hltb_single.extract_words(50)\n",
    "        \n",
    "# Printing significant words and corresponding t-values\n",
    "for word_list in [words_owners_with_hltb_single, words_uscore_with_hltb_single]:\n",
    "    for word, tv1 in word_list:\n",
    "        print(f\"Word: {word}, t-values: {tv1}\")\n",
    "\n",
    "## Plotting most influential words using NLPAnalyzer:\n",
    "## Single plot for prediction of owners\n",
    "NLP_owners_with_hltb_single.plot_words()\n",
    "## Single plot for prediction of owners\n",
    "NLP_uscore_with_hltb_single.plot_words()\n",
    "## Combined plot for prediction of owners and user score\n",
    "NLP_owners_with_hltb_single.plot_words_multi(words_owners_with_hltb_single, words_uscore_with_hltb_single, \"Owners\", \"User Score\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b067e7c-aef3-4d18-910e-cba1f89c30e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
