{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a576fee5-dff0-4fd1-b447-b575d70096e4",
   "metadata": {},
   "source": [
    "# Notebook: Basic NLP Analyses\n",
    "\n",
    "## Structure of notebook\n",
    "#### Step 1: Imports, functions and classes\n",
    "#### Step 2: Data preparation\n",
    "#### Step 3: NLP basics\n",
    "#### Step 4: Vectorization, analysis and basic plots\n",
    "#### Step 5: Printing and plotting most influential words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e2732-d47e-425a-9036-10fb75b24e8f",
   "metadata": {},
   "source": [
    "## Step 1: Imports, functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26eee37a-c9ad-4a24-a573-a54505842c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "## dependencies: spacy, en_core_web_sm, nltk stopwords\n",
    "####################################\n",
    "\n",
    "## General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Imports for NLP\n",
    "import nltk, re, spacy, string\n",
    "from spacy.lang.en.examples import sentences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "## Imports for analyses\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "\n",
    "## Function for encoding\n",
    "\n",
    "def oh_encoder(df, column):\n",
    "    \"\"\"Simple one-hot encoder that takes a dataframe and a column name and returns the dataframe with encoded column.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    all_categories = list(set(g for categories in df[column] for g in categories))\n",
    "    one_hot_df = pd.DataFrame(0, index=df.index, columns=all_categories)\n",
    "    for i, categories in enumerate(df[column]):\n",
    "            one_hot_df.loc[i, categories] = 1\n",
    "    df = df.drop(columns=[column]).join(one_hot_df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## Function for data preparation\n",
    "\n",
    "def data_preparation(df):\n",
    "    \"\"\"Take a dataframe, prepare it for use in NLP and analyses and return prepared dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (Dataframe): Original dataframe to be prepared\n",
    "        \n",
    "    Returns:\n",
    "        df (Dataframe): Prepared dataframe\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    ###############################    \n",
    "    ## Data Cleaning & Recoding\n",
    "    ###############################\n",
    "    \n",
    "    ## drop columns not used in analyses\n",
    "    df.drop(['sid', 'store_url', 'store_promo_url', 'published_meta', 'published_stsp', 'published_hltb',\n",
    "           'published_igdb', 'image', 'current_price', 'discount', \n",
    "           'gfq_url', 'gfq_difficulty_comment', 'gfq_rating_comment', 'gfq_length_comment',\n",
    "           'hltb_url', 'meta_url', 'igdb_url'], axis=1, inplace=True)\n",
    "    \n",
    "    ## publish date as timedelta\n",
    "    df[\"published_store\"] = pd.to_datetime(df[\"published_store\"]) - pd.Timestamp(1997, 1, 1)\n",
    "    df[\"published_store\"] = df[\"published_store\"].apply(lambda value: value.days)\n",
    "     \n",
    "    ## missing data 1: If language or voiceover is missing, set to \"One_unknown\"\n",
    "    df.loc[df[\"languages\"].isna(), \"languages\"] = \"One_unknown\"\n",
    "    df.loc[df[\"voiceovers\"].isna(), \"voiceovers\"] = \"One_unknown\"\n",
    "\n",
    "    ## delete games without English as language:\n",
    "    count_no_en = 0\n",
    "    for x in df.index:\n",
    "        if \"english\" not in df.loc[x,\"languages\"].lower():\n",
    "            count_no_en += 1\n",
    "            df = df.drop(labels=x, axis=0)\n",
    "    print(f\"Games without English language: {count_no_en}\")\n",
    "             \n",
    "    ## use only number of languages and voiceovers\n",
    "    df[\"languages\"] = df[\"languages\"].apply(lambda value: len(value.split(\",\")))\n",
    "    df[\"voiceovers\"] = df[\"voiceovers\"].apply(lambda value: len(value.split(\",\")))\n",
    "      \n",
    "    ## missing data 2: drop columns with more than 75% missing data:\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > df.shape[0]*0.75:\n",
    "            df.drop(col, axis=1, inplace=True) \n",
    "    \n",
    "    ## One-Hot-Encoding\n",
    "    \n",
    "    ## Genres\n",
    "    ## split strings in genre and platform columns\n",
    "    df['genres'] = df['genres'].apply(lambda x: x.split(','))\n",
    "    df['platforms'] = df['platforms'].apply(lambda x: x.split(','))\n",
    "    ## replace genres\n",
    "    df['genres'] = df['genres'].apply(lambda genres: list(set(['Indie' if genre == 'Инди' else genre for genre in genres])))\n",
    "    df['genres'] = df['genres'].apply(lambda genres: list(set(['Adventure' if genre == 'Приключенческие игры' else genre for genre in genres])))\n",
    "    \n",
    "    ## One-Hot Encoding\n",
    "    df = oh_encoder(df, \"genres\")\n",
    "    df = oh_encoder(df, \"platforms\")\n",
    "    \n",
    "    ## Rename columns including spaces\n",
    "    df.rename(columns={'Game Development':'Game_Development',\n",
    "                      'Free to Play':'Free_to_Play',\n",
    "                      'Massively Multiplayer':'Massively_Multiplayer',\n",
    "                      'Early Access':'Early_Access',\n",
    "                      'Sexual Content':'Sexual_Content'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## Function for text cleaning\n",
    "\n",
    "def text_cleaner(sentence):\n",
    "    \"\"\"Take a string, clean it for use in vectorization and return cleaned string.\n",
    "    \n",
    "    Args:\n",
    "        sentence (string): Original string to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        doc_str (string): Cleaned String\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ## counter\n",
    "    global call_count \n",
    "    call_count += 1\n",
    "    if call_count%500 == 0:\n",
    "        print(call_count)\n",
    "    if sentence is None:\n",
    "        doc_str = \"\"\n",
    "    else:\n",
    "        ## tokenize and delete pronouns, stopwords and punctuation\n",
    "        doc = nlp(sentence)\n",
    "        clean_doc = [token.lemma_.lower() for token in doc if (token.pos_ !=\"PRON\") and (token.lemma_ not in stopWords) and (token.lemma_ not in punctuations)]\n",
    "        ## rejoin texts\n",
    "        doc_str = \" \".join(clean_doc)\n",
    "        ## deleting points, tabs, spaces and line breaks\n",
    "        doc_str = re.sub(\"[\\s]+\", \" \", doc_str)\n",
    "        ## deleting numbers\n",
    "        doc_str = re.sub(r'\\d+', '', doc_str) \n",
    "    return doc_str\n",
    "\n",
    "\n",
    "## Class for analyses\n",
    "\n",
    "class NLPAnalyzer():\n",
    "    \"\"\"A class used for the analyses including NLP.\n",
    "\n",
    "        Attributes:\n",
    "\n",
    "            df (Dataframe):\n",
    "                The DataFrame to be used for analyses.\n",
    "            target_var (string):\n",
    "                The name of the target column.\n",
    "            max_feature_list (list):\n",
    "                A list of interger values to be used as maximum number of features for vectorization.\n",
    "            test_size (float):\n",
    "                The fraction of the Dataframe to be used as the test sample.\n",
    "            tfidf (bool):\n",
    "                A Boolean that is True if TF-IDF vectorization should be used instead of BOW/Count vectorization.\n",
    "            var_dict (dictionary):\n",
    "                A dictionary of potential targets with column names (keys) and column descriptions (values).\n",
    "                These columns are excluded from analyses and only the target used in target_var is included as target.\n",
    "            train_data (Dataframe):\n",
    "                Data to be used as train features. Generated when runnning :meth: \"analyze\".\n",
    "            test_data (Dataframe):\n",
    "                Data to be used as test features. Generated when runnning :meth: \"analyze\".\n",
    "            train_target (Dataframe):\n",
    "                Data to be used as train target. Generated when runnning :meth: \"analyze\".\n",
    "            test_target (Dataframe):\n",
    "                Data to be used as test target. Generated when runnning :meth: \"analyze\".\n",
    "            vectorizer (instance of vectorizer class from sklearn):\n",
    "                Vectorizer used. Can be CountVectorizer or TfidfVectorizer. Generated when runnning :meth: analyze.\n",
    "            model_sm (instance of statsmodels.api.OLS):\n",
    "                OLS regression used for extraction of t-values. Generated when runnning :meth: analyze.\n",
    "        \n",
    "        Methods:\n",
    "    \n",
    "            analyze():\n",
    "                Conduct analyses, print figures for Adjusted R-squared and T-values of non-NLP features. Return table of results.\n",
    "  \n",
    "            extract_words(max_features=50):\n",
    "                Conduct OLS regression for max_features and return list of words showing significant effects and corresponding t-values.\n",
    "                \n",
    "            plot_words_multi(word_list_1, word_list_2, target_1, target_2, top_number=25):\n",
    "                Plot words of two analyses extracted with :meth: extract_words.\n",
    "                       \n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_var, max_feature_list=[0, 2000, 2500, 3000, 3500, 4000], test_size=0.25, tfidf=False):\n",
    "        \"\"\"Construct all the necessary attributes for the NLPAnalyzer object.\n",
    "\n",
    "        Args:\n",
    "            df (Dataframe):\n",
    "                The DataFrame to be used for analyses.\n",
    "            target_var (string):\n",
    "                The name of the target column.\n",
    "            max_feature_list (list):\n",
    "                A list of interger values to be used as maximum number of features for vectorization.\n",
    "            test_size (float):\n",
    "                The fraction of the Dataframe to be used as the test sample.\n",
    "            tfidf (bool):\n",
    "                A Boolean that is True if TF-IDF vectorization should be used instead of BOW/Count vectorization.\n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.target_var = target_var\n",
    "        self.test_size = test_size\n",
    "        self.max_feature_list = max_feature_list\n",
    "        self.tfidf = tfidf\n",
    "        self.var_dict={\"stsp_owners\":\"Owners\", \"store_uscore\":\"User Score\", \"igdb_popularity\":\"Popularity\"}\n",
    "\n",
    "        print(\"*\"*50, \"\\n\", \"Initialization - Target:\", self.target_var, \"\\n\", \"*\"*50)\n",
    "        \n",
    "        ## delete unused variables\n",
    "        self.df = self.df.drop([\"name\", \"developers\", \"publishers\", \"categories\", \"tags\", \"achievements\", \"gfq_rating\", \"description\"], axis=1)\n",
    "\n",
    "        # Variable-specific drops: delete other DVs\n",
    "        all_vars = [\"store_uscore\", \"stsp_owners\", \"igdb_popularity\"]\n",
    "        drop_vars = [var for var in all_vars if var != self.target_var]\n",
    "        self.df = self.df.drop(drop_vars, axis=1)\n",
    "        \n",
    "        ## delete missings\n",
    "        self.df = self.df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "        ## Reset index\n",
    "        self.df.reset_index()\n",
    "        \n",
    "        print(\"*\"*50, \"\\n\", \"Data preparation done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Conduct analyses, print figures for Adjusted R-squared and T-values of non-NLP features. Return list of results.     \n",
    "            \n",
    "            Returns:\n",
    "                results_df (Dataframe):\n",
    "                    Dataframe of models used, maximum number of features and corresponding values of adjusted R-squared.\n",
    "                    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Train-test split\n",
    "        self.train_data, self.test_data, self.train_target, self.test_target = train_test_split(\n",
    "            self.df.drop([self.target_var], axis=1), self.df[self.target_var], test_size=self.test_size, random_state=42)\n",
    "        \n",
    "        # List for results\n",
    "        results = []\n",
    "\n",
    "        # Standardize numeric features\n",
    "        columns_to_scale = ['published_store', 'full_price', 'languages', 'voiceovers', 'hltb_single']\n",
    "        scaler = StandardScaler()\n",
    "        self.train_data[columns_to_scale] = scaler.fit_transform(self.train_data[columns_to_scale])\n",
    "        self.test_data[columns_to_scale] = scaler.transform(self.test_data[columns_to_scale])\n",
    "\n",
    "        \n",
    "        # Loop for different values of max_features\n",
    "        for max_feat in self.max_feature_list:\n",
    "            try:\n",
    "                if max_feat > 0:\n",
    "\n",
    "                    if self.tfidf:\n",
    "                        self.vectorizer = TfidfVectorizer(\n",
    "                            stop_words='english',\n",
    "                            max_df=0.9,\n",
    "                            min_df=10,\n",
    "                            max_features=max_feat\n",
    "                        )\n",
    "\n",
    "                        print(\"*\"*50, \"\\n\", \"Using TF-IDF Vectorizer\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                    else:\n",
    "                        self.vectorizer = CountVectorizer(\n",
    "                            stop_words='english',\n",
    "                            max_df=0.9,\n",
    "                            min_df=10,\n",
    "                            max_features=max_feat\n",
    "                        )\n",
    "                        \n",
    "                        print(\"*\"*50, \"\\n\", \"Using BOW Vectorizer\", \"\\n\", \"*\"*50)\n",
    "                    \n",
    "                    # Fit and transformation\n",
    "                    X_train_text = self.vectorizer.fit_transform(self.train_data['description_clean_nonum'])\n",
    "                    X_test_text = self.vectorizer.transform(self.test_data['description_clean_nonum'])\n",
    "                    \n",
    "                    # Convert text features into dataframe\n",
    "                    X_train_text_df = pd.DataFrame(X_train_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.train_data.index)\n",
    "                    X_test_text_df = pd.DataFrame(X_test_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.test_data.index)\n",
    "                else:\n",
    "                    # When max_feat is 0, use only non-text features\n",
    "                    X_train_text_df = pd.DataFrame()\n",
    "                    X_test_text_df = pd.DataFrame()\n",
    "               \n",
    "                # Keep non-text features\n",
    "                X_train_non_text = self.train_data.drop('description_clean_nonum', axis=1)\n",
    "                X_test_non_text = self.test_data.drop('description_clean_nonum', axis=1)\n",
    "                \n",
    "                # Make sure all non-text features are numeric\n",
    "                X_train_non_text = X_train_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                X_test_non_text = X_test_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                \n",
    "                # Concatenate dataframes\n",
    "                X_train = pd.concat([X_train_non_text, X_train_text_df], axis=1)\n",
    "                X_test = pd.concat([X_test_non_text, X_test_text_df], axis=1)\n",
    "                \n",
    "                # Make sure all features are numeric\n",
    "                X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "                X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "       \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Vectorization done\", \"\\n\", \"*\"*50)\n",
    "                \n",
    "                \n",
    "                # OLS with statsmodels\n",
    "                self.model_sm = sm.OLS(self.train_target, sm.add_constant(X_train)).fit()\n",
    "                adj_r2_sm = self.model_sm.rsquared_adj\n",
    "                if max_feat == 0:\n",
    "                    display(self.model_sm.summary())\n",
    "                print(\"*\"*50, \"\\n\", \"R-squared for Statsmodels OLS (train data):\", self.model_sm.rsquared)\n",
    "                print(\"*\"*50, \"\\n\", \"Adjusted R-squared for Statsmodels OLS (train data):\", adj_r2_sm)\n",
    "                \n",
    "                # Add constant to X_test for prediction\n",
    "                X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "                r2_sm_test = r2_score(self.test_target, self.model_sm.predict(X_test_const))\n",
    "                adj_r2_sm_test = 1 - ( ( (1-r2_sm_test) * (len(self.test_target) - 1) ) / ( (len(self.test_target) - X_test_const.shape[1] - 1) ) )\n",
    "                print(\"*\"*50, \"\\n\", \"R-squared for Statsmodels OLS (test data):\", r2_sm_test)    \n",
    "                print(\"*\"*50, \"\\n\", \"Adjusted R-squared for Statsmodels OLS (test data):\", adj_r2_sm_test)                          \n",
    "                results.append(('OLS', max_feat, adj_r2_sm_test))\n",
    "              \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - OLS SM done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # OLS with sklearn\n",
    "                model_lr = LinearRegression(fit_intercept=True).fit(X_train, self.train_target)\n",
    "                r2_lr = r2_score(self.test_target, model_lr.predict(X_test))\n",
    "                adj_r2_lr = 1 - (1-r2_lr)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('LinearRegression', max_feat, adj_r2_lr))\n",
    "               \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - OLS SK done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # Lasso\n",
    "                model_lasso = Lasso(fit_intercept=True, alpha=1e4).fit(X_train, self.train_target)\n",
    "                r2_lasso = r2_score(self.test_target, model_lasso.predict(X_test))\n",
    "                adj_r2_lasso = 1 - (1-r2_lasso)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('Lasso', max_feat, adj_r2_lasso))\n",
    "                \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Lasso done\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                \n",
    "                # Ridge\n",
    "                model_ridge = Ridge(fit_intercept=True, alpha=1e4).fit(X_train, self.train_target)\n",
    "                r2_ridge = r2_score(self.test_target, model_ridge.predict(X_test))\n",
    "                adj_r2_ridge = 1 - (1-r2_ridge)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('Ridge', max_feat, adj_r2_ridge))\n",
    "               \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Ridge done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "        \n",
    "                # ElasticNet\n",
    "                model_ela = ElasticNet(fit_intercept=True, alpha=1e4).fit(X_train, self.train_target)\n",
    "                r2_ela = r2_score(self.test_target, model_ela.predict(X_test))\n",
    "                adj_r2_ela = 1 - (1-r2_ela)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('ElasticNet', max_feat, adj_r2_ela))\n",
    "                \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - ElasticNet done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "                \n",
    "                # RandomForestRegressor\n",
    "                model_rf = RandomForestRegressor(n_estimators=200).fit(X_train, self.train_target)\n",
    "                r2_rf = r2_score(self.test_target, model_rf.predict(X_test))\n",
    "                adj_r2_rf = 1 - (1-r2_rf)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('RandomForest', max_feat, adj_r2_rf))\n",
    "\n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - RF done\", \"\\n\", \"*\"*50)\n",
    "\n",
    "                \n",
    "                # SVR\n",
    "                model_svr = SVR(gamma=\"auto\", C=1e3).fit(X_train, self.train_target)\n",
    "                r2_svr = r2_score(self.test_target, model_svr.predict(X_test))\n",
    "                adj_r2_svr = 1 - (1-r2_svr)*(len(self.test_target)-1)/(len(self.test_target)-X_test.shape[1]-1)\n",
    "                results.append(('SVR', max_feat, adj_r2_svr))\n",
    "       \n",
    "                print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - SVR done\", \"\\n\", \"*\"*50)\n",
    "                \n",
    "            \n",
    "            except ValueError as e:\n",
    "                print(f\"Error with max_features={max_feat}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Convert results into dataframe\n",
    "        self.results_df = pd.DataFrame(results, columns=['Model', 'Max_Features', 'Adjusted_R2'])\n",
    "\n",
    "        # Plot adjusted R-squared for different max features\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        for model in self.results_df['Model'].unique()[1:]:\n",
    "            subset = self.results_df[self.results_df['Model'] == model]\n",
    "            plt.plot(subset['Max_Features'], subset['Adjusted_R2'], label=model, alpha=0.5)\n",
    "    \n",
    "        plt.xlabel('Max Features')\n",
    "        plt.ylabel('Adjusted R-squared')\n",
    "        plt.title(f'Adjusted R-squared for Different Models (predicting {self.var_dict[self.target_var]})')\n",
    "        plt.xticks(self.max_feature_list)\n",
    "        plt.legend()\n",
    "        plt.savefig(f'plots/fig_{self.target_var}_R2.png')\n",
    "    \n",
    "        # Plot t-values for non-text variables as horizontal bar plot\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        non_text_t_values = self.model_sm.tvalues[1:len(X_train_non_text.columns) + 1]\n",
    "        non_text_feature_names = X_train_non_text.columns  # Get non-text feature names\n",
    "        plt.barh(non_text_feature_names, non_text_t_values, color='skyblue')\n",
    "    \n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Non-text Features')\n",
    "        plt.title(f'T-values for Non-text Variables (predicting {self.var_dict[self.target_var]})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/fig_{self.target_var}_Tvalues.png')\n",
    "    \n",
    "        return self.results_df\n",
    "\n",
    "    \n",
    "    def extract_words(self, max_features=50):\n",
    "        \"\"\"Conduct OLS regression for max_features and return list of significant words and corresponding t-values.\n",
    "                \n",
    "            Args:      \n",
    "                max_features (int):\n",
    "                    maximum number of features to be used for extraction of words showing significant effects.\n",
    "                        \n",
    "            Returns:\n",
    "                significant_words (list):\n",
    "                    List of words showing significant effects and corresponding t-values.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        max_feat = max_features\n",
    "        \n",
    "        # OLS for t-value extraction with specific number of words\n",
    "        try:\n",
    "            if max_feat > 0:\n",
    "\n",
    "                if self.tfidf:\n",
    "                    self.vectorizer = TfidfVectorizer(\n",
    "                        stop_words='english',\n",
    "                        max_df=0.9,\n",
    "                        min_df=5,\n",
    "                        max_features=max_feat\n",
    "                    )\n",
    "\n",
    "                    print(\"*\"*50, \"\\n\", \"Using TF-IDF Vectorizer\", \"\\n\", \"*\"*50)\n",
    "        \n",
    "                else:\n",
    "                    self.vectorizer = CountVectorizer(\n",
    "                        stop_words='english',\n",
    "                        max_df=0.9,\n",
    "                        min_df=5,\n",
    "                        max_features=max_feat\n",
    "                    )\n",
    "                        \n",
    "                    print(\"*\"*50, \"\\n\", \"Using BOW Vectorizer\", \"\\n\", \"*\"*50)\n",
    "                    \n",
    "                # Fit and transformation\n",
    "                X_train_text = self.vectorizer.fit_transform(self.train_data['description_clean_nonum'])\n",
    "                X_test_text = self.vectorizer.transform(self.test_data['description_clean_nonum'])\n",
    "                    \n",
    "                # Convert text features into dataframe\n",
    "                X_train_text_df = pd.DataFrame(X_train_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.train_data.index)\n",
    "                X_test_text_df = pd.DataFrame(X_test_text.toarray(), columns=self.vectorizer.get_feature_names_out(), index=self.test_data.index)\n",
    "            \n",
    "            else:\n",
    "                # When max_feat is 0, word extraction is not conducted\n",
    "                print(\"There are no influential words if maximum number of text features is zero.\")\n",
    "                return self\n",
    "               \n",
    "            # Keep non-text features\n",
    "            X_train_non_text = self.train_data.drop('description_clean_nonum', axis=1)\n",
    "            X_test_non_text = self.test_data.drop('description_clean_nonum', axis=1)\n",
    "                \n",
    "            # Make sure all non-text features are numeric\n",
    "            X_train_non_text = X_train_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "            X_test_non_text = X_test_non_text.apply(pd.to_numeric, errors='coerce')\n",
    "                \n",
    "            # Concatenate dataframes\n",
    "            X_train = pd.concat([X_train_non_text, X_train_text_df], axis=1)\n",
    "            X_test = pd.concat([X_test_non_text, X_test_text_df], axis=1)\n",
    "                \n",
    "            # Make sure all features are numeric\n",
    "            X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "            X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "      \n",
    "            print(\"*\"*50, \"\\n\", \"NLP:\", max_feat, \"words - Vectorization done\", \"\\n\", \"*\"*50)\n",
    "              \n",
    "            # OLS with statsmodels\n",
    "            self.model_sm = sm.OLS(self.train_target, sm.add_constant(X_train)).fit()\n",
    "                         \n",
    "        except ValueError as e:\n",
    "            print(f\"Error with max_features={max_feat}: {e}\")\n",
    "      \n",
    "        # Extract significant words\n",
    "        significant_words = []\n",
    "        text_feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        for i, feature_name in enumerate(text_feature_names):\n",
    "            t_value = self.model_sm.tvalues[len(X_train_non_text.columns) + i + 1]\n",
    "            \n",
    "            if abs(t_value) > 1.96:\n",
    "                significant_words.append((feature_name, t_value))\n",
    "        \n",
    "        return significant_words\n",
    "\n",
    "\n",
    "    def plot_words(self, number_words=50, max_words=25):\n",
    "        \"\"\"Plot words with significant influence on target variable in OLS regression.\n",
    "\n",
    "        Args:\n",
    "            number_words (int):\n",
    "                Number of words to be used in OLS regression for extraction of words.\n",
    "            max_words (int):\n",
    "                Maximum number of words to be plotted.\n",
    "\n",
    "        Returns:\n",
    "            top (list):\n",
    "                List of top words with highest t-values. Length is based on max_words.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Sort word lists based on t-values in descending order\n",
    "        sorted_words = sorted(self.extract_words(number_words), key=lambda x: x[1], reverse=True)\n",
    "        sorted_words = sorted(self.extract_words(number_words), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top words\n",
    "        top = sorted_words[:max_words]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Extract words and t-values\n",
    "        words = [entry[0] for entry in top]\n",
    "        t_values = [entry[1] for entry in top]\n",
    "        \n",
    "        # Make barh plot\n",
    "        plt.barh(words, t_values, color='green', label=self.target_var.title(), alpha=0.5)\n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Words')\n",
    "        plt.title(f'T-values of most influential words in models for {self.target_var.title()}')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/fig_word_t-values_{self.target_var}.png')\n",
    "\n",
    "        return top\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_words_multi(word_list_1, word_list_2, target_1, target_2, max_words=25):\n",
    "        \"\"\"Plot words of two analyses extracted with :meth: extract_words.\n",
    "\n",
    "        Args:\n",
    "            words_list_1 (list):\n",
    "                List of words and corresponding t-values extracted with :meth: extract_words. \n",
    "            words_list_1 (list):\n",
    "                List of words and corresponding t-values extracted with :meth: extract_words. \n",
    "            target_1 (string):\n",
    "                Name of dependent variable predicted with word_list_1. \n",
    "            target_2 (string):\n",
    "                Name of dependent variable predicted with word_list_2. \n",
    "            max_words (int):\n",
    "                Maximum number of words to be extracted and plotted.\n",
    "\n",
    "            Returns:\n",
    "                top_1 (list):\n",
    "                    List of top words with highest t-values from word_list_1. Length is based on :attr: max_words.\n",
    "                top_2 (list):\n",
    "                    List of top words with highest t-values from word_list_1. Length is based on :attr: max_words.\n",
    "                \n",
    "        \"\"\"\n",
    "\n",
    "        # Sort word lists based on t-values in descending order\n",
    "        sorted_words_1 = sorted(word_list_1, key=lambda x: x[1], reverse=True)\n",
    "        sorted_words_2 = sorted(word_list_2, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract top words\n",
    "        top_1 = sorted_words_1[:max_words]\n",
    "        top_2 = sorted_words_2[:max_words]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Extract words and t-values\n",
    "        words_1 = [entry[0] for entry in top_1]\n",
    "        t_values_1 = [entry[1] for entry in top_1]\n",
    "        words_2 = [entry[0] for entry in top_2]\n",
    "        t_values_2 = [entry[1] for entry in top_2]\n",
    "        \n",
    "        # Make barh plot\n",
    "        plt.barh(words_1, t_values_1, color='green', label=target_1, alpha=0.5)\n",
    "        plt.barh(words_2, t_values_2, color='blue', label=target_2, alpha=0.5)\n",
    "        plt.xlabel('T-values')\n",
    "        plt.ylabel('Words')\n",
    "        plt.title('T-values of most influential words in models for {target_1} and {target_2}')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/fig_word_t-values_{target_1.replace(\" \", \"\")}_{target_2.replace(\" \", \"\")}.png')\n",
    "\n",
    "        return top_1, top_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7eccd-056a-4e97-b680-fff01ae036a1",
   "metadata": {},
   "source": [
    "## Step 2: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5bda8f-6c55-4787-b69f-caa128a4fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games without English language: 1931\n"
     ]
    }
   ],
   "source": [
    "## Read data\n",
    "df = pd.read_json(\"../../data/steamdb.json\")\n",
    "\n",
    "## Prepare data\n",
    "df = data_preparation(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e8eda-0f66-4bff-b478-da4292ebdde5",
   "metadata": {},
   "source": [
    "## Step 3: NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432eb81e-4c35-47ff-94ad-0f0125d176aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Starting text cleaner\n",
      "**************************************************\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#########################################\n",
    "## Users might need to manually download stopwords:\n",
    "# nltk.download('stopwords')\n",
    "#########################################\n",
    "\n",
    "## Clean \"description\"\n",
    "stopWords = stopwords.words(\"english\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "## Applying text_cleaner to description\n",
    "print(\"*\"*50 + \"\\nStarting text cleaner\\n\" + \"*\"*50) \n",
    "## Adding a global counter to print in text_cleaner function since it takes a long time\n",
    "call_count = 0\n",
    "## Using text_cleaner\n",
    "df[\"description_clean_nonum\"] = df[\"description\"].apply(text_cleaner)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3268e2-8e2a-41d1-9a50-3243f20ddb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## optional: save as pickle for fast reload\n",
    "# df.to_pickle(\"data/df_with_lemmas_compressed.pkl\", compression='bz2')\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14de48-3808-4e5d-8567-8766dc2033c5",
   "metadata": {},
   "source": [
    "## Step 4: Vectorization, analysis and basic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e91641-7d1d-44f4-a5c2-0d02b8f39828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## optional: if saved as pickle, load it here:\n",
    "# df = pd.read_pickle(\"data/df_with_lemmas_compressed.pkl\", compression='bz2')\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5eb2f-a821-4bc3-bb2a-268a82c770ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Use NLPAnalyzer class to conduct basic analyses and save basic plots of results\n",
    "\n",
    "NLP_owners = NLPAnalyzer(df, \"stsp_owners\", [0, 25, 50, 100, 150, 200, 250])\n",
    "results_owners = NLP_owners.analyze()\n",
    "\n",
    "NLP_uscore = NLPAnalyzer(df, \"store_uscore\", [0, 25, 50, 100, 150, 200, 250])\n",
    "results_uscore = NLP_uscore.analyze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224c859-b749-4500-99a2-9160a8656e95",
   "metadata": {},
   "source": [
    "## Step 5: Printing and plotting most influential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d61a01-9fd9-43c7-834e-16381c901b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Extract words using NLPAnalyzer:\n",
    "words_owners = NLP_owners.extract_words(50)\n",
    "words_uscore= NLP_uscore.extract_words(50)\n",
    "        \n",
    "# Printing significant words and corresponding t-values\n",
    "for word_list in [words_owners, words_uscore]:\n",
    "    for word, tv1 in word_list:\n",
    "        print(f\"Word: {word}, t-values: {tv1}\")\n",
    "\n",
    "## Plotting most influential words using NLPAnalyzer:\n",
    "## Single plot for prediction of owners\n",
    "NLP_owners.plot_words()\n",
    "## Single plot for prediction of owners\n",
    "NLP_uscore.plot_words()\n",
    "## Combined plot for prediction of owners and user score\n",
    "NLP_owners.plot_words_multi(words_owners, words_uscore, \"Owners\", \"User Score\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
