{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdddc0-02ea-4718-b0f2-f73b6f8a4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Laden des DataFrames und Setzen der ersten Spalte nicht als Index\n",
    "df = pd.read_json(r\"C:/Users/gueld/Desktop/Portfolio Projekt/Daten/github/steamdb.json\")\n",
    "\n",
    "# Überprüfen auf fehlende Werte in der Spalte 'full_price' (full_price und current_price haben gleich viele missing values)\n",
    "missing_full_price = df[df['full_price'].isnull()]\n",
    "\n",
    "# Ausgeben der Einträge der Spalte 'store_url' bei fehlenden Werten in 'full_price'\n",
    "missing_urls = missing_full_price['store_url']\n",
    "\n",
    "# Funktion zum Laden der URLs aus der Datenbankdatei mit fehlenden Werten in 'full_price' und 'current_price'\n",
    "def load_missing_urls(database_path):\n",
    "    df = pd.read_json(database_path)\n",
    "    missing_full_price = df[df['full_price'].isnull() & df['current_price'].isnull()]\n",
    "    missing_urls = missing_full_price['store_url'].tolist()\n",
    "    return missing_urls\n",
    "\n",
    "# Funktion zum Scrapen einer Steam-Seite\n",
    "def scrape_steam_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        data = {}\n",
    "        \n",
    "        title_element = soup.find('div', class_='apphub_AppName')\n",
    "        data['name'] = title_element.get_text() if title_element else 'N/A'\n",
    "        \n",
    "        current_price_element = soup.find('div', class_='game_purchase_price')\n",
    "        data['current_price'] = current_price_element.get_text().strip() if current_price_element else 'N/A'\n",
    "        \n",
    "        full_price_element = soup.find('div', class_='discount_original_price')\n",
    "        data['full_price'] = full_price_element.get_text().strip() if full_price_element else 'N/A'\n",
    "        \n",
    "        data['store_url'] = url\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Fehler beim Laden der Seite: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Funktion zum Speichern von Daten in einer JSON-Datei\n",
    "def save_to_json(data, file_index, output_dir):\n",
    "    file_path = os.path.join(output_dir, f'steam_data_{file_index}.json')\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Gespeichert: {file_path}\")\n",
    "\n",
    "# Pfade und Einstellungen\n",
    "database_path = r\"C:/Users/gueld/Desktop/Portfolio Projekt/Daten/github/steamdb.json\"\n",
    "output_dir = r\"C:/Users/gueld/Desktop/Portfolio Projekt/Daten/steam_scrape_json/neu/steam_price.json\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lade die URLs aus der Datenbank mit fehlenden Werten in 'full_price' und 'current_price'\n",
    "urls = load_missing_urls(database_path)\n",
    "\n",
    "# Initialisiere Variablen - speichere jeweils 500 zeilen in einer .json Datei\n",
    "scraped_data = []\n",
    "file_index = 1\n",
    "batch_size = 500\n",
    "\n",
    "# Iteriere über die URLs und scrape jede Seite\n",
    "for url in urls:\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    data = scrape_steam_page(url)\n",
    "    if data:\n",
    "        scraped_data.append(data)\n",
    "    \n",
    "    # Speichere die Daten in einer JSON-Datei, wenn die Batch-Größe erreicht ist\n",
    "    if len(scraped_data) >= batch_size:\n",
    "        save_to_json(scraped_data, file_index, output_dir)\n",
    "        scraped_data = []  # Leere die Liste für den nächsten Batch\n",
    "        file_index += 1\n",
    "\n",
    "# Speichere verbleibende Daten in einer letzten JSON-Datei\n",
    "if scraped_data:\n",
    "    save_to_json(scraped_data, file_index, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab0735-f786-4cfc-95df-2119afded2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
